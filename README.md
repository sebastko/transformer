# transformer
PyTorch implementation of the transformer architecture from the ``[Attention Is All You Need](https://arxiv.org/abs/1706.03762)'' paper, Vaswani et al 2017. Implemented for fun and to further my own understanding of the paper. As of 9/12/2022, only the encoder portion of the architecture is implemented and I haven't tested it thoroughly yet, so use with caution.

TODO:
 * Implement decoder (as of 9/12, only the encoder is implemented)
 * Implement training loop
 * Validate that network learns something correctly (perhaps try to reproduce some simple problem)
